{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-19T18:58:40.782148500Z",
     "start_time": "2024-09-19T18:58:37.623593Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.core.indices.vector_store import VectorIndexRetriever\n",
    "from pinecone import Pinecone\n",
    "from llama_index.embeddings.nvidia import NVIDIAEmbedding\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from llama_index.llms.nvidia import NVIDIA\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from dotenv import load_dotenv\n",
    "from transformers import GPT2Tokenizer\n",
    "from llama_index.core.query_pipeline import (\n",
    "    QueryPipeline,\n",
    "    InputComponent,\n",
    "    ArgPackComponent,\n",
    ")\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.postprocessor.colbert_rerank import ColbertRerank\n",
    "from typing import Any, Dict, List, Optional\n",
    "from llama_index.core.bridge.pydantic import Field\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.query_pipeline import CustomQueryComponent\n",
    "from llama_index.core.schema import NodeWithScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Load API keys\n",
    "nvidia_api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Check if API keys are correctly loaded\n",
    "if not nvidia_api_key or not pinecone_api_key:\n",
    "    raise EnvironmentError(\"NVIDIA or Pinecone API key not found. Check your .env file.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-19T18:58:40.788275500Z",
     "start_time": "2024-09-19T18:58:40.784149Z"
    }
   },
   "id": "e7d4ae9aa7750d0b"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Project_\\Aarogyam\\server\\aarogyam-ml-server\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer for text chunking\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Function to truncate or split text into chunks of max 512 tokens\n",
    "def truncate_text(text, max_tokens=500):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "    return tokenizer.convert_tokens_to_string(tokens)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-19T18:58:41.807842300Z",
     "start_time": "2024-09-19T18:58:41.250964600Z"
    }
   },
   "id": "9dc09841e313bc4a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Setup NVIDIA embedding and LLM with proper error handling\n",
    "try:\n",
    "    Settings.text_splitter = SentenceSplitter(chunk_size=400)\n",
    "    Settings.embed_model = NVIDIAEmbedding(api_key=nvidia_api_key)\n",
    "    Settings.llm = NVIDIA(model='meta/llama3-70b-instruct', api_key=nvidia_api_key)\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up NVIDIA model: {str(e)}\")\n",
    "    exit(1)\n",
    "\n",
    "try:\n",
    "    pc = Pinecone(api_key=pinecone_api_key)\n",
    "    pinecone_index = pc.Index(\"aarogyam-chat-rag\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Pinecone: {str(e)}\")\n",
    "    exit(1)\n",
    "    \n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-19T18:58:44.378203Z",
     "start_time": "2024-09-19T18:58:42.349244400Z"
    }
   },
   "id": "7b0e0fb990c594ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "# Load documents and truncate text properly\n",
    "documents = SimpleDirectoryReader(\"../rag_data/md\").load_data()\n",
    "\n",
    "# Modify the text content of document objects without breaking them\n",
    "for doc in documents:\n",
    "    doc.text = truncate_text(doc.text)  # Modify the document's text in place\n",
    "\n",
    "# Set up Pinecone Vector Store and Storage Context\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Create Vector Store Index from processed documents\n",
    "try:\n",
    "    index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
    "except Exception as e:\n",
    "    print(f\"Error creating index: {str(e)}\")\n",
    "    exit(1)\n",
    "\n",
    "# Convert index to query engine\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Example query\n",
    "response = query_engine.query(\"What is ayurveda?\")\n",
    "print(response)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a5be6dfefe9e30"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "input_component = InputComponent()\n",
    "\n",
    "rewrite = (\n",
    "    \"Please generate a query for a semantic search engine based on the current conversation related to Ayurveda.\\n\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    \"{chat_history_str}\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    \"Latest message: {query_str}\\n\"\n",
    "    'Query:\"\"\"\\n'\n",
    ")\n",
    "rewrite_template = PromptTemplate(rewrite)\n",
    "\n",
    "argpack_component = ArgPackComponent()\n",
    "retriever = VectorIndexRetriever(index=index, similarity_top_k=5)\n",
    "reranker = ColbertRerank(top_n=3)\n",
    "\n",
    "DEFAULT_CONTEXT_PROMPT = (\n",
    "    \"Here is some Ayurvedic context that might be helpful:\\n\"\n",
    "    \"-----\\n\"\n",
    "    \"{node_context}\\n\"\n",
    "    \"-----\\n\"\n",
    "    \"Please formulate a response to the following question using the provided Ayurvedic context.\"\n",
    "    \"{query_str}\\n\"\n",
    ")\n",
    "\n",
    "class ResponseWithChatHistory(CustomQueryComponent):\n",
    "    llm: NVIDIA = Field(..., description=\"NVIDIA LLM\")\n",
    "    system_prompt: Optional[str] = Field(\n",
    "        default=None, description=\"System prompt to use for the LLM\"\n",
    "    )\n",
    "    context_prompt: str = Field(\n",
    "        default=DEFAULT_CONTEXT_PROMPT,\n",
    "        description=\"Context prompt to use for the LLM\",\n",
    "    )\n",
    "\n",
    "    def _validate_component_inputs(\n",
    "            self, input: Dict[str, Any]\n",
    "    ) -> Dict[str, Any]:\n",
    "        return input\n",
    "\n",
    "    @property\n",
    "    def _input_keys(self) -> set:\n",
    "        return {\"chat_history\", \"nodes\", \"query_str\"}\n",
    "\n",
    "    @property\n",
    "    def _output_keys(self) -> set:\n",
    "        return {\"response\"}\n",
    "\n",
    "    def _prepare_context(\n",
    "            self,\n",
    "            chat_history: List[ChatMessage],\n",
    "            nodes: List[NodeWithScore],\n",
    "            query_str: str,\n",
    "    ) -> List[ChatMessage]:\n",
    "        node_context = \"\"\n",
    "        for idx, node in enumerate(nodes):\n",
    "            node_text = node.get_content(metadata_mode=\"llm\")\n",
    "            node_context += f\"Context Chunk {idx}:\\n{node_text}\\n\\n\"\n",
    "\n",
    "        formatted_context = self.context_prompt.format(\n",
    "            node_context=node_context, query_str=query_str\n",
    "        )\n",
    "        user_message = ChatMessage(role=\"user\", content=formatted_context)\n",
    "\n",
    "        chat_history.append(user_message)\n",
    "\n",
    "        if self.system_prompt is not None:\n",
    "            chat_history = [\n",
    "                               ChatMessage(role=\"system\", content=self.system_prompt)\n",
    "                           ] + chat_history\n",
    "\n",
    "        return chat_history\n",
    "\n",
    "    def _run_component(self, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Run the component.\"\"\"\n",
    "        chat_history = kwargs[\"chat_history\"]\n",
    "        nodes = kwargs[\"nodes\"]\n",
    "        query_str = kwargs[\"query_str\"]\n",
    "    \n",
    "        prepared_context = self._prepare_context(\n",
    "            chat_history, nodes, query_str\n",
    "        )\n",
    "    \n",
    "        response = self.llm.chat(prepared_context)\n",
    "        # Use the entire response as it is\n",
    "        return {\"response\": response}\n",
    "\n",
    "\n",
    "    async def _arun_component(self, **kwargs: Any) -> Dict[str, Any]:\n",
    "        chat_history = kwargs[\"chat_history\"]\n",
    "        nodes = kwargs[\"nodes\"]\n",
    "        query_str = kwargs[\"query_str\"]\n",
    "\n",
    "        prepared_context = self._prepare_context(\n",
    "            chat_history, nodes, query_str\n",
    "        )\n",
    "\n",
    "        response = await self.llm.achat(prepared_context)\n",
    "\n",
    "        return {\"response\": response}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-19T19:06:05.586832500Z",
     "start_time": "2024-09-19T19:06:04.144918Z"
    }
   },
   "id": "82b925b3201d3991"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "response_component = ResponseWithChatHistory(\n",
    "    llm=NVIDIA(api_key=nvidia_api_key),\n",
    "    system_prompt=(\n",
    "        \"You are a Q&A system specializing in Ayurveda. You will be provided with previous chat history and relevant context \"\n",
    "        \"to help in answering user queries related to Ayurvedic practices, remedies, and wellness.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "pipeline = QueryPipeline(\n",
    "    modules={\n",
    "        \"input\": input_component,\n",
    "        \"rewrite_template\": rewrite_template,\n",
    "        \"llm\": Settings.llm,\n",
    "        \"rewrite_retriever\": retriever,\n",
    "        \"query_retriever\": retriever,\n",
    "        \"join\": argpack_component,\n",
    "        \"reranker\": reranker,\n",
    "        \"response_component\": response_component,\n",
    "    },\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# run both retrievers -- once with the hallucinated query, once with the real query\n",
    "pipeline.add_link(\n",
    "    \"input\", \"rewrite_template\", src_key=\"query_str\", dest_key=\"query_str\"\n",
    ")\n",
    "pipeline.add_link(\n",
    "    \"input\",\n",
    "    \"rewrite_template\",\n",
    "    src_key=\"chat_history_str\",\n",
    "    dest_key=\"chat_history_str\",\n",
    ")\n",
    "pipeline.add_link(\"rewrite_template\", \"llm\")\n",
    "pipeline.add_link(\"llm\", \"rewrite_retriever\")\n",
    "pipeline.add_link(\"input\", \"query_retriever\", src_key=\"query_str\")\n",
    "\n",
    "# each input to the argpack component needs a dest key -- it can be anything\n",
    "# then, the argpack component will pack all the inputs into a single list\n",
    "pipeline.add_link(\"rewrite_retriever\", \"join\", dest_key=\"rewrite_nodes\")\n",
    "pipeline.add_link(\"query_retriever\", \"join\", dest_key=\"query_nodes\")\n",
    "\n",
    "# reranker needs the packed nodes and the query string\n",
    "pipeline.add_link(\"join\", \"reranker\", dest_key=\"nodes\")\n",
    "pipeline.add_link(\n",
    "    \"input\", \"reranker\", src_key=\"query_str\", dest_key=\"query_str\"\n",
    ")\n",
    "\n",
    "# synthesizer needs the reranked nodes and query str\n",
    "pipeline.add_link(\"reranker\", \"response_component\", dest_key=\"nodes\")\n",
    "pipeline.add_link(\n",
    "    \"input\", \"response_component\", src_key=\"query_str\", dest_key=\"query_str\"\n",
    ")\n",
    "pipeline.add_link(\n",
    "    \"input\",\n",
    "    \"response_component\",\n",
    "    src_key=\"chat_history\",\n",
    "    dest_key=\"chat_history\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-19T19:06:06.814708Z",
     "start_time": "2024-09-19T19:06:06.809311900Z"
    }
   },
   "id": "23c9c02200734631"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only one output is supported.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 16\u001B[0m\n\u001B[0;32m     14\u001B[0m chat_history \u001B[38;5;241m=\u001B[39m pipeline_memory\u001B[38;5;241m.\u001B[39mget()\n\u001B[0;32m     15\u001B[0m chat_history_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([\u001B[38;5;28mstr\u001B[39m(x) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m chat_history])\n\u001B[1;32m---> 16\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquery_str\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchat_history\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchat_history\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchat_history_str\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchat_history_str\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m user_msg \u001B[38;5;241m=\u001B[39m ChatMessage(role\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m, content\u001B[38;5;241m=\u001B[39mmsg)\n\u001B[0;32m     22\u001B[0m pipeline_memory\u001B[38;5;241m.\u001B[39mput(user_msg)\n",
      "File \u001B[1;32mD:\\Project_\\Aarogyam\\server\\aarogyam-ml-server\\venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:265\u001B[0m, in \u001B[0;36mDispatcher.span.<locals>.wrapper\u001B[1;34m(func, instance, args, kwargs)\u001B[0m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_enter(\n\u001B[0;32m    258\u001B[0m     id_\u001B[38;5;241m=\u001B[39mid_,\n\u001B[0;32m    259\u001B[0m     bound_args\u001B[38;5;241m=\u001B[39mbound_args,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    262\u001B[0m     tags\u001B[38;5;241m=\u001B[39mtags,\n\u001B[0;32m    263\u001B[0m )\n\u001B[0;32m    264\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 265\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    266\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    267\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevent(SpanDropEvent(span_id\u001B[38;5;241m=\u001B[39mid_, err_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(e)))\n",
      "File \u001B[1;32mD:\\Project_\\Aarogyam\\server\\aarogyam-ml-server\\venv\\Lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:413\u001B[0m, in \u001B[0;36mQueryPipeline.run\u001B[1;34m(self, return_values_direct, callback_manager, batch, *args, **kwargs)\u001B[0m\n\u001B[0;32m    409\u001B[0m     query_payload \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mdumps(\u001B[38;5;28mstr\u001B[39m(kwargs))\n\u001B[0;32m    410\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mevent(\n\u001B[0;32m    411\u001B[0m     CBEventType\u001B[38;5;241m.\u001B[39mQUERY, payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mQUERY_STR: query_payload}\n\u001B[0;32m    412\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m query_event:\n\u001B[1;32m--> 413\u001B[0m     outputs, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    414\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    415\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_values_direct\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_values_direct\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    416\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshow_intermediates\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    417\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    418\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    419\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    421\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[1;32mD:\\Project_\\Aarogyam\\server\\aarogyam-ml-server\\venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:265\u001B[0m, in \u001B[0;36mDispatcher.span.<locals>.wrapper\u001B[1;34m(func, instance, args, kwargs)\u001B[0m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_enter(\n\u001B[0;32m    258\u001B[0m     id_\u001B[38;5;241m=\u001B[39mid_,\n\u001B[0;32m    259\u001B[0m     bound_args\u001B[38;5;241m=\u001B[39mbound_args,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    262\u001B[0m     tags\u001B[38;5;241m=\u001B[39mtags,\n\u001B[0;32m    263\u001B[0m )\n\u001B[0;32m    264\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 265\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    266\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    267\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevent(SpanDropEvent(span_id\u001B[38;5;241m=\u001B[39mid_, err_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(e)))\n",
      "File \u001B[1;32mD:\\Project_\\Aarogyam\\server\\aarogyam-ml-server\\venv\\Lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:785\u001B[0m, in \u001B[0;36mQueryPipeline._run\u001B[1;34m(self, return_values_direct, show_intermediates, batch, *args, **kwargs)\u001B[0m\n\u001B[0;32m    779\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    780\u001B[0m     result_output_dicts, intermediate_dicts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_multi(\n\u001B[0;32m    781\u001B[0m         {root_key: kwargs}, show_intermediates\u001B[38;5;241m=\u001B[39mshow_intermediates\n\u001B[0;32m    782\u001B[0m     )\n\u001B[0;32m    784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m--> 785\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_single_result_output\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    786\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult_output_dicts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_values_direct\u001B[49m\n\u001B[0;32m    787\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m    788\u001B[0m         intermediate_dicts,\n\u001B[0;32m    789\u001B[0m     )\n",
      "File \u001B[1;32mD:\\Project_\\Aarogyam\\server\\aarogyam-ml-server\\venv\\Lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:718\u001B[0m, in \u001B[0;36mQueryPipeline._get_single_result_output\u001B[1;34m(self, result_outputs, return_values_direct)\u001B[0m\n\u001B[0;32m    711\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Get result output from a single module.\u001B[39;00m\n\u001B[0;32m    712\u001B[0m \n\u001B[0;32m    713\u001B[0m \u001B[38;5;124;03mIf output dict is a single key, return the value directly\u001B[39;00m\n\u001B[0;32m    714\u001B[0m \u001B[38;5;124;03mif return_values_direct is True.\u001B[39;00m\n\u001B[0;32m    715\u001B[0m \n\u001B[0;32m    716\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    717\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(result_outputs) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 718\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOnly one output is supported.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    720\u001B[0m result_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28miter\u001B[39m(result_outputs\u001B[38;5;241m.\u001B[39mvalues()))\n\u001B[0;32m    721\u001B[0m \u001B[38;5;66;03m# return_values_direct: if True, return the value directly\u001B[39;00m\n\u001B[0;32m    722\u001B[0m \u001B[38;5;66;03m# without the key\u001B[39;00m\n\u001B[0;32m    723\u001B[0m \u001B[38;5;66;03m# if it's a dict with one key, return the value\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: Only one output is supported."
     ]
    }
   ],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "pipeline_memory = ChatMemoryBuffer.from_defaults(token_limit=8000)\n",
    "\n",
    "user_inputs = [\n",
    "    \"Hello! Can you tell me about Ayurvedic practices?\",\n",
    "    \"What are some common Ayurvedic remedies for headaches?\",\n",
    "    \"How does Ayurveda approach digestive health?\",\n",
    "    \"Can you suggest some Ayurvedic treatments for stress relief?\",\n",
    "    \"Thanks for the information! What is the role of diet in Ayurveda?\",\n",
    "]\n",
    "\n",
    "for msg in user_inputs:\n",
    "    chat_history = pipeline_memory.get()\n",
    "    chat_history_str = \"\\n\".join([str(x) for x in chat_history])\n",
    "    response = pipeline.run(\n",
    "        query_str=msg,\n",
    "        chat_history=chat_history,\n",
    "        chat_history_str=chat_history_str,\n",
    "    )\n",
    "    user_msg = ChatMessage(role=\"user\", content=msg)\n",
    "    pipeline_memory.put(user_msg)\n",
    "    print(str(user_msg))\n",
    "    pipeline_memory.put(response.message)\n",
    "    print(str(response.message))\n",
    "    print()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-19T19:06:24.556042800Z",
     "start_time": "2024-09-19T19:06:07.557939Z"
    }
   },
   "id": "b5833dfd364c61df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e6f4afaed2d584e6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
